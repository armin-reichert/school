\section{Grammars and derivations}

In the introduction of this book we already learned about formal languages. The
wish to describe these in general infinite sets of words by a finite generating
system leads to the notion of a {\bf grammar}.

\begin{definition}[Chomsky grammar]
$G = (N, T, P, s)$ is a {\bf Chomsky grammar}, if
\begin{enumerate}
  \item $N$ is a finite, nonempty set of {\bf nonterminal symbols}
  \item $T$ is a finite, nonempty set of {\bf terminal symbols} with
  $N \cap T = \emptyset$
  \item $P \subset N^+ \times (N \cup T)^*$ is a finite set of {\bf productions}
  \item $S \in N$ is the {\bf axiom} or {\bf start symbol} 
\end{enumerate}
\end{definition}

Notation: For $p = (u, v) \in P$ we also write $u \stackrel{p}{\to} v$ and
$Q(p) = u, Z(p) = v$ denote the source and target of a production.

Examples:
\begin{enumerate}
  \item $G_1 = (N, T, P, S)$ with $N = \{ S \}, T = \{ x, x' \},$ \\ 
  $P = \{ S \to SS, S \to xSx', S \to \epsilon \}$
  \item $G_2 = (N, T, P, S)$ with $N = \{ S, X \}, T = \{ x, x' \}$ \\
  $P = \{ S \to xSX, S \to xX, X \to x'S, X \to x' \}$
\end{enumerate}

To use a grammar for generating the words of a language, starting with the axiom
there are intermediate words generated by application of the productions, until
the produced word will contain terminal symbols only. This leads to the notation
of a ''derivation'' which we will now define formally.

\begin{definition}[directly derivable, derivable]
Let $G = (N, T, P, S)$ be a grammar and let $w, w' \in (N \cup T)^*$.

$w'$ is {\bf directly derivable} from $w$ in $G$, notation: $w
\dderives{G} w'$, if there are segmentations $w = w_1 \cdot u \cdot w_2$ and
$w' = w_1 \cdot v \cdot w_1$ and a production $(u, v) \in P$.

$w'$ is {\bf derivable} from $w$, notation: $w \derives{G} w'$, if there exists
a sequence of words \[ w = w_0, \ldots, w_n = w',\quad n \in \mathbb{N}, w_i \in
(N \cup T)^* \] such that for each $0 \leq i \leq n : w_i \dderives{G}
w_{i+1}$.
\end{definition}

Such a sequence is called a {\bf derivation} of length $n$. $Q$ and $Z$ can be
extended in a natural way to derivations.

A derivation is called {\bf canonic} or {\bf leftmost}, if in each step $w
\dderives{G} w'$ it holds:

If $w = w_1 \cdot u \cdot w_2$ and $w' = w_1 \cdot v \cdot w_2$ are the
segmentations and $(u, v) \in P$ the applied production, then $w_1 \in T^*$
which means that always the leftmost nonterminal is replaced.

If the grammar is known we omit the index $G$ from the symbols $\dderives{G}$
and $\derives{G}$.

Let us consider some properties of the relation $\derives{G}$:

\begin{lemma}
Let $G$ be a grammar. Then the following holds:
\begin{enumerate}
  \item $(u, v) \in P \Rightarrow u \derives{G} v$
  \item $w \derives{G} w$ (reflexivity)
  \item $w \derives{G} w' \wedge w' \derives{G} w'' \Rightarrow w \derives{G}
  w''$ (transitivity)
  \item $w_1 \derives{G} w'_1 \wedge w_2 \derives{G} w'_2 \Rightarrow w_1 \cdot
  w_2 \derives{G} w'_1 \cdot w'_2$ (compatibility with monoid operation)
\end{enumerate}
Here $w, w', w'', w_1, w_2, w'_1, w'_2 \in (N \cup T)^*$.
\end{lemma}

Proof:
\begin{enumerate}
  \item follows from the definition of $\derives{G}$.
  \item clear with $n = 0$ in the definition of $\derives{G}$.
  \item There exist sequences $w = w_0, \ldots, w_n = w',\ w' = w'_0, \ldots,
  w'_m = w''$ with $w_i \derives{G} w_{i+1}$ and $w'_j \derives{G} w'_{j+1}$.
  Because $w_n = w' = w'_0$, the composed sequence $w = w_0, \cdots, w_n, w'_1,
  \cdots, w'_n = w''$ is a derivation from $w$ to $w''$.
  \item Exercise for the reader
\end{enumerate}

Notation: An intermediate word that is generated by a derivation starting with
the axiom is called a {\bf sentence form} of $G$. We define:

\begin{definition}
\[ SF(G) := \{ w \in (N \cup T)^*\ |\ S \derives{G} w \} \] is the set of {\bf
sentence forms} of $G$.
\end{definition}

Now we are able to define the formal language generated by a grammar.

\begin{definition}
Let $G = (N, T, P, S)$ be a grammar. \[ L(G) := \{ w \in T^*\ |\ S \derives{G} w
\} \] is the {\bf language generated by} $G$.
\end{definition}

Note: $L(G) = SF(G) \cap T^*$.

Examples:
\begin{enumerate}
  \item One can see that for the grammar $G_1$ given in the previous example 1
  it holds: $L(G_1)$ is the Dyck language over the alphabet $\{ x, x' \}$.
  \item Let $G = (N, T, P, S)$ with $N = \{ S \}, T = \{ a, b \}, P = \{ S \to
  aSb, S \to \epsilon \}$. Then $L(G) = \{ a^n b^n \ |\ n \in \mathbb{N}$.
\end{enumerate}

The simple proof is left to the reader.

Grammars are compared with relation to the languages they generate. We define:

\begin{definition}[weak grammar equivalence]
$G$ is {\bf weakly equivalent} to $G' \Leftrightarrow L(G) = L(G')$.
\end{definition}

Remark: The reader should convince himself that the grammars $G_1$ and $G_2$
from the first example generate the same language.

Of course you can define infinitely many different grammars for each language.

We now can define different classes of grammars (and languages generated by
these) depending on certain restrictions of their production system. In the next
chapter we will meet the so called {\em right-linear} grammars and their
languages.

Special importance, also from a practical point of view, have the so called
{\em contextfree} grammars.

\begin{definition}[context-free grammar]
A grammar $G = (N, T, P, S)$ is called {\bf context-free} if $P \subset N \times
(N \cup T)^*$.
\end{definition}

The term ''context-free'' describes the fact that in a sentence-form a
nonterminal may be replaced by the right-hand side of a production without need
to respect the ''context'' to the left and right around that nonterminal symbol.

In chapter 4 we will treat context-free grammars in depth.






























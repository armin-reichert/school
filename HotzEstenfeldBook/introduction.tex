\chapter*{Introduction}

There are several reasons for the interest in the theory of formal languages in
computer science. Practical problems as they arise in the context of definition
and translation of programming languages find an exact description in the theory
of formal languages and thus get accessible to an exact treatment. Generation
processes definable by formal languages can be interpreted as non-deterministic
automata which is as a generalized notion of a computer.

These kinds of generalizations in general are easier to understand than
deterministic algorithms which contain more details that do not reflect the
original problem but the necessity to uniquely define the algorithm. This is
part of the reason for the difficulty to prove the correctness of programs in an
understandable way. The proof of correctness for grammars or other mechanisms for generating
languages on the other side offers the possibility to study correctness proofs
on simpler objects.

The theory of formal languages in this respect contains the theory of
algorithms but most often only the theory of context-free languages is treated
because of her extraordinary simplicity and beauty.

In the spotlight of the theory are different methods for defining
formal language classes, to study their word and equivalence problems and to
put them into different hierarchical classifications.

The generation processes themselves become objects of interest in the theory
because the generation process of a language in case of programming languages
relates to the semantics of programs.

Of course, in the context of such a pocket book we have to make a strong
selection of topics concerning language classes, generation processes as well as
basic questions. In doing so, we let us guide by the intention to keep the
formal machinery rather small.

Because the theory of finite automata is the foundation for the whole theory of
formal languages, we start our book with this topic. In developing this theory
we do not consider the technical realization of finite automata by logical
circuits and binary storage devices but rather focus on the basic algorithm however it is realized.
 
Our intuitive notion of finite automaton consists of a finite, oriented graph whose 
edges are labeled with the symbols from the input alphabet of the automaton. 
Depending on the input word, we look for a path in the graph labeled with that
word. If the end point of such a path, originating from the dedicated ''start point'' 
of the automaton, is a member of the set of ''end points'', our automaton
''accepts'' the word and doesn't so otherwise.

We prove the equivalence of this concept with the other known methods of
defining finite automata. We prove the usual closure properties of languages
defined by finite automata. Additionally we investigate the relation between
deterministic and non-deterministic automata and also 2-way automata.

It is possible to generalize this theory in the direction of considering not
only the free monoid of strings (words) over a finite alphabet but also
arbitrary monoids.

By considering finite automata with output, which means to attach a second label
at the graph edges, we get the theory of rational transducers. An extensive
treatment of the theory of general transductions can be found in the book by
Berstel.

Here we restrict ourselves to some special generalizations of the free monoid
(of words), namely the {\bf free group}, the {\bf H-group} (where the relation
$x x^{-1} = 1$ holds for $x$ from the generating system, but not $x^{-1} x = 1$) and the
{\bf polycyclic monoid} (in addition to $x x^{-1} = 1$ it holds $x y^{-1} = 0$
for $x \neq y$ and $0 x = x 0 = 0$ for $x,y$ from the generating system).

By investigating the transductions from free monoids into the polycyclic
monoids one gets a smooth transition from the theory of finite automata into the
theory of context-free languages.

The corresponding construction of the theory of context-free languages leads to
a simple path to the most important representation theorems. This includes the
theorems of Chomsky-SchÃ¼tzenberger, Shamir and Greibach. Also for the
transformation into Greibach normal form we get a simple and efficient
algorithm.

In the same easy way as for finite automata you can prove the known closure
properties for context-free languages.

In the end we also prove the equivalence of this representation with the usual
representation of context-free languages using context-free grammars.

Our composition of the theory is very close to the one repeatedly recommended by
Goldstine since 1977, but its origin is independent from him. The difference is
that we prove Greibach's representation theorem by making our automaton deterministic,
namely by switching from output monoids to monoid rings. Doing that you get the
theorem of Shamir in a natural way and from this the theorem of Greibach.

From the theorem of Shamir you can get quite easily the algorithm of Valiant for
deciding the word problem of context-free languages. Because of lack of space
this could not be included into this book, the same holds for the treatment of
the  deterministic languages.

We want to emphasize another advantage of this composition of the theory: As
known, the exact formalization of the notion of ''derivation'' when using
grammars brings some difficulties. In our theory, the ''derivation tree''
corresponds to a path in our graph.

Maybe the use of non-free monoids is initially a problem for readers not used to
it. But it seems to be the case that defining context-free
languages in that way supports the intuition. For example, the usage of ''syntax
diagrams'' for the definition of programming languages gives some evidence for
this.

Because we judge the former as rather important, we want to explain it on a
specific example, namely the so-called {\bf Dyck language}.

The {\bf Dyck language} $D(X_k)$ contains the correctly nested bracket sequences
over $k$ different pairs of brackets, where $k \in \mathbb{N}$.

A formal definition of $D(X_k)$ is as follows:

Let $X_k = \{ x_1, \ldots, x_k \}$ be an alphabet of $k$ elements. Define
$\bar{X}_k = \{ \bar{x}_1, \ldots, \bar{x}_k \}$ such that $\bar{x}_i$ is regarded as the
closing bracket for $x_i$.

Then it holds:

\begin{enumerate}
  \item $\epsilon \in D(X_k)$
  \item $u, v \in D(X_k) \Rightarrow u \cdot v \in D(X_k)$
  \item $u \in D(X_k) \Rightarrow x_i \cdot u \cdot \bar{x}_i \in D(X_k),\quad
  \forall i = 1, \ldots, k$
  \item $D(X_k)$ is minimal with (1), (2) and (3). 
\end{enumerate}

For $D(X_k)$ we get the following syntax diagram:

\begin{center}
\begin{tikzpicture}[node distance = 10em]
\tikzset{VertexStyle/.append style = {minimum size = 1em}}
\tikzset{EdgeStyle/.append style = {->, bend left}}
\tikzset{LabelStyle/.style = {fill=white}}
\node[VertexStyle](S){S};
\node[VertexStyle,right=of S](V){ }; 
\node[VertexStyle,right=of V](F){F};
\draw[EdgeStyle](S) to node[LabelStyle]{$s_4/\epsilon$} (F);
\draw[EdgeStyle](F) to node[LabelStyle]{$s_1/\epsilon$} (S);
\path[]
	(S)	edge[bend left] 	node[LabelStyle] {$s_{x_1}/x_1$} (V)
	(S)	edge 							node[LabelStyle] {$\cdots$} (V)
	(S)	edge[bend right] 	node[LabelStyle] {$s_{x_k}/x_k$} (V)
	(V)	edge[bend left] 	node[LabelStyle] {$s_{\bar{x}_1}/\bar{x}_1$} (F)
	(V)	edge 							node[LabelStyle] {$\cdots$} (F)
	(V)	edge[bend right] 	node[LabelStyle] {$s_{x_k}/x_k$} (F)
	(V) edge							node[LabelStyle] {$s_2/\epsilon$} (S)
	;
\end{tikzpicture}
\end{center}

If we consider all labelings of paths from $S$ to $F$ we get of course also
words not contained in $D(X_k)$, for example $x_1 x_2 \bar{x}_k$ or $x_1
\bar{x}_1 \bar{x}_2$ etc.

We have to guarantee that we get Dyck words only. To do that, we define a
homomorphism from the path category of the graph into the polycyclic monoid over
$X_k \cup \bar{X}_k$ such that the homomorphic images of the paths from $S$ to
$F$ have a special form, for example to be equal to the unit of the
polycyclic monoid.

Let us consider the word
\[ x_1 x_2 \bar{x}_2 \bar{x}_1 x_2 \bar{x}_2 \in D(X_2), \]
then we have different paths
\[s_{x_1} s_2 s_{x_2} s_{\bar{x}_2} s_3 s_{\bar{x}_1} s_1
s_{x_2} s_{\bar{x}_2} \] 
and 
\[s_{x_1} s_2 s_{x_2} s_{\bar{x}_2} s_3
s_{\bar{x}_1} s_3 s_2 s_{x_2} s_{\bar{x}_2}\]
which both have this word as their labeling and we can easily define a
homomorphism in the sense above.

We get different paths in our graph leading to acceptance of the same word.

The problem to construct a graph such that for each word in the accepted
language exactly one path exists leads to the existence of the {\bf
deterministic} finite automaton with storage.
